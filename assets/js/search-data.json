{
  
    
        "post0": {
            "title": "First steps in regression (OLS and Gradient descent)",
            "content": "Exploring regression . In this notebook, I want to start exploring regression. While python libraries provide many convenient ways to run a regression on a dataset, I want to start with a manual approach. The intent here is to cement my understanding of this topic . #importing import numpy as np import plotly.graph_objects as go import plotly.express as px #I will use one of the statsmodels API, not sure which yet import statsmodels.api as sm import statsmodels.formula.api as smf import pandas as pd from IPython.display import display_html, HTML from io import StringIO . I am going to generate a simple linear dataset with one variable. I will then apply the following methods to arrive at the relationship of y with x 1) Using the Ordinary Least Squares Method 2) Using Gradient descent . #This generator function will generate data simple linear datasets def genData(n = 20, slope = 1, intercept = 2, noise = False): x = np.linspace(1,n,n) y = slope*x + intercept if noise: y += np.random.normal(0,1,n) dataset = pd.DataFrame({&#39;x&#39;:x, &#39;y&#39;: y}) return x, y, dataset . #Lets start by using OLS in stats model to run a regression slope = 2.5 intercept = -0.65 _,_,dataset = genData(n = 20, slope = slope, intercept = intercept, noise = True) linear_regression = smf.ols(formula = &#39;y ~ x&#39;, data = dataset).fit() linear_regression.summary() . OLS Regression Results Dep. Variable: y | R-squared: 0.997 | . Model: OLS | Adj. R-squared: 0.997 | . Method: Least Squares | F-statistic: 6976. | . Date: Thu, 13 May 2021 | Prob (F-statistic): 9.20e-25 | . Time: 08:08:22 | Log-Likelihood: -21.939 | . No. Observations: 20 | AIC: 47.88 | . Df Residuals: 18 | BIC: 49.87 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept -0.6273 | 0.355 | -1.768 | 0.094 | -1.373 | 0.118 | . x 2.4743 | 0.030 | 83.523 | 0.000 | 2.412 | 2.536 | . Omnibus: 2.672 | Durbin-Watson: 2.779 | . Prob(Omnibus): 0.263 | Jarque-Bera (JB): 1.733 | . Skew: 0.720 | Prob(JB): 0.420 | . Kurtosis: 2.940 | Cond. No. 25.0 | . Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. #Lets now plot this relationship on a graph to get a visual understanding of the data #using HTML to display plotly in fastpages fig = go.Figure() fig.add_trace(go.Scatter(x = dataset.x, y = dataset.y, mode=&#39;markers&#39;, name=&quot;Original Data&quot;)) fig.add_trace(go.Scatter(x = dataset.x, y = linear_regression.predict(dataset.x), line_color = &#39;orange&#39;, name=&quot;Regression Line&quot;)) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . THe OLS formula is well know and is used to miniminse SSE (sum of squared errors). Starting with the linear regression formula: . $$ hat{y}_i = beta_0 + beta_1x_i$$ . In this expression $ hat{y}_i$ is the estimate for the true $y_i$. Our task is to find $ beta_0$ and $ beta_1$ that minimises the mean of the squared errors (MSE) given by the following formula . $$MSE = frac{1}{n} displaystyle sum_{i=1}^{n} (y_i - hat{y}_i)^2$$ . Substituting $ hat{y}_i$ we get . $$MSE = frac{1}{n} displaystyle sum_{i=1}^{n} (y_i - beta_0 - beta_1x_i)^2$$ . Taking the partial derivatives of the equation above with respect to $ beta_0$ and $ beta_1$ and setting them to zero, we get the following . $$ beta_0 = bar{y} - beta_1 bar{x}$$ . $$ beta_1 = frac{ displaystyle sum_{i=1}^{n}(x_i - bar{x})(y_i - bar{y})}{ displaystyle sum_{i=1}^{n} (x_i - bar{x})^2} = rho_{xy} frac{ sigma_y}{ sigma_x}$$ . Note: One of the many sources . #Defining a function that returns beta0 and beta1 for a given x and y arrays def OLS(x,y): rhoxy = np.corrcoef(y, x) sigmax = np.std(x) sigmay = np.std(y) beta1 = rhoxy[0,1]*(sigmay/sigmax) beta0 = np.mean(y) - beta1*(np.mean(x)) return beta0, beta1 result = OLS(dataset.x, dataset.y) print (&quot;Beta0 = %0.4f&quot; %result[0]) print(&quot;Beta1 = %0.4f&quot; %result[1]) . Beta0 = -0.6273 Beta1 = 2.4743 . The values calculated by this formula match with the results of the statsmodel linear regression. . Now onto the fun part Gradient Descent . Gradient Descent . There are tons of resources available that do a great job of explaining gradient descent. In summary, gradient descent is an iterative method that allows us to minimize the loss function. It may not be the best method for a single predictor linear regression since we have a well defined formula to calculate the cofficients. However for other applications there are loss functions that are more complex. In those situations, gradient descent makes the job faster. . Our goal with gradient descent is to traven down a gradient of the loss function and arrive at the hopefully global minimum of the loss function. It may be the case that gradient descent will arrive at a gobal minimum. However as I understand it, for $MSE$ in linear regression, there is only 1 global minimum. . We will start by plotting the $MSE$ function to evaluate its shape. To do that, we have to calculate the value of the $MSE$ for various values of $ beta_0$ and $ beta_1$ . from sklearn.metrics import mean_squared_error #For a given range of b0 and b1, this function will return a 2-d array of the SSE def mse_calc(y, x, interceptRange, slopeRange, size=20): intercept = np.linspace(interceptRange[0],interceptRange[1],size) slope = np.linspace(slopeRange[0],slopeRange[1],size) obs = len(y) z_vals = np.zeros((size,size)) #Calculating the SSE for each combination of slope and intercept. TODO: Vectorize this for i in range(0,size): for j in range(0,size): y_pred = np.multiply(slope[j],x) + intercept[i] z_vals[i][j] = mean_squared_error(y, y_pred) return intercept, slope, z_vals . #Lets plot a contour map of the MSE to get a visual understanding of the problem that we have to solve print(&quot;Results from OLS&quot;) print(&quot;Intercept = &quot;, round(result[0],2)) print(&quot;Slope = &quot;, round(result[1],2)) interceptRange = [intercept - 2, intercept + 2] slopeRange = [slope - 3,slope + 3] surface_x, surface_y, surface_z = mse_calc(y = dataset.y, x = dataset.x, interceptRange = interceptRange, slopeRange = slopeRange, size = 50) fig = go.Figure(data = [go.Contour(z = surface_z, x = surface_x, y = surface_y, colorscale = &#39;Bluyl&#39;)]) fig.add_trace(go.Scatter(x = [result[0]], y = [result[1]], mode=&#39;markers&#39;, hoverinfo = &#39;skip&#39;)) fig.update_layout( title=&quot;Contour map of loss function&quot;, xaxis_title=&quot;Intercept&quot;, yaxis_title=&quot;Slope&quot;) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . Results from OLS Intercept = -0.63 Slope = 2.47 . . . Now that we have visualised the loss function, the next step is to implement our gradient descent algorithm and plot its path to understand how it works. Note: Great explanation here . As discussed, gradient descent is an iterative formula which works by progressively moving in the direction of a local/global mimina of the loss function. In the case of linear regression with 1 predictor ($ hat{y}_i = beta_0 + beta_1x_i$), we would need to calculate the partial detivative of the loss function with respect to $ beta_0$ and $ beta_1$. We will then use a learning rate $ alpha$ to calculate the new value of $ beta_0$ and $ beta_1$. In this case, the partial derivatives are: . $$ frac{ partial beta_0}{ partial MSE} = frac{-2}{n} displaystyle sum_{i=1}^{n} (y_i - hat{y}_i)$$ $$ frac{ partial beta_1}{ partial MSE} = frac{-2}{n} displaystyle sum_{i=1}^{n} x_i(y_i - hat{y}_i)$$ . We start with randomly selected values of $ beta_0$ and $ beta_1$, generate $ hat{Y}$ and then use the following to update the values: . $$ beta_0 = beta_0 - alpha* frac{ partial beta_0}{ partial MSE}$$ $$ beta_1 = beta_1 - alpha* frac{ partial beta_1}{ partial MSE}$$ . We stop when either of the following conditions is met: 1) The sucessive differences between the iterations fall below a treshold 2) OR We have iterated an n number of times (this n us user defined and I dont know of any rules of thumb here yet) . I will start with a function that calculates this for us. Then I will plot this for an intutive understading . def gradient_descent(y,x,epochs = 10, treshold = 0.001, learning_rate = 0.0001, beta0 = 3, beta1 = 1): obs = float(len(y)) learning_rate = learning_rate*(-2/obs) beta0path = [beta0] beta1path = [beta1] for i in range(0,epochs): y_pred = beta1*x + beta0 beta0_adj = learning_rate*np.sum(y - y_pred) beta0 = beta0 - beta0_adj #For large observations, this equation causes an overflow. I have tried to figure it out, but have not #found a robust way of managing it. This works fine for ~20 obs. So it will do for our purposes beta1_adj = learning_rate*np.sum(x*(y - y_pred)) beta1 = beta1 - beta1_adj beta0path.append(beta0) beta1path.append(beta1) return beta0, beta1, np.array(beta0path), np.array(beta1path) print(&quot;Intercept (x) = &quot;,result[0]) print(&quot;Slope (y) = &quot;, result[1]) . Intercept (x) = -0.6273023448439652 Slope (y) = 2.4742514126098554 . #x, y, _ = genData(n = 20, slope = 54, intercept = 0.456) beta0, beta1, beta0path, beta1path = gradient_descent(dataset.y,dataset.x,epochs = 3500, learning_rate = 0.005, beta0 = -1.5, beta1 = 4) . fig = go.Figure(data = [go.Contour(z = surface_z, x = surface_x, y = surface_y, colorscale = &#39;Bluyl&#39;)]) fig.add_trace(go.Scatter(x = beta0path, y = beta1path, mode=&#39;lines&#39;)) fig.add_hline(y = result[1] ,line_dash = &#39;dash&#39;, line_width = 0.5, line_color = &quot;Blue&quot;) fig.add_vline(x = result[0] ,line_dash = &#39;dash&#39;, line_width = 0.5, line_color = &quot;Blue&quot;) fig.update_layout( title=&quot;Contour map of loss function&quot;, xaxis_title=&quot;Intercept&quot;, yaxis_title=&quot;Slope&quot;) fig.update_xaxes(range=interceptRange) fig.update_yaxes(range=slopeRange) fig.show() . The chart above shows the path gradient descent takes to &#39;arrive at the solution. The other variable of interest in gradient descent is $ alpha$ (the learning rate). In my experimentation so far, I have observed a high sensitivity to this variable. Too low and we dont arrive at a solution, too high and it jumps around and misses the solution. The charts below show how different learning rates impact the progression of the variables to the true value . from plotly.subplots import make_subplots intercept = 2.5 slope = 4.5 epochs = 1200 x, y, dataset = genData(n = 20, slope = slope, intercept = intercept) beta0guess = 0 beta1guess = -10 learningRates = [0.0001,0.0065, 0.007] beta0results = np.zeros((3, epochs+1)) beta1results = np.zeros((3, epochs+1)) #Step 1: Generate learning paths for 3 different learning rates: Too cold, just right and too hot _, _, beta0results[0], beta1results[0] = gradient_descent(dataset.y,dataset.x,epochs = epochs, learning_rate = learningRates[0], beta0 = beta0guess, beta1 = beta1guess) _, _, beta0results[1], beta1results[1] = gradient_descent(dataset.y,dataset.x,epochs = epochs, learning_rate = learningRates[1], beta0 = beta0guess, beta1 = beta1guess) _, _, beta0results[2], beta1results[2] = gradient_descent(dataset.y,dataset.x,epochs = epochs, learning_rate = learningRates[2], beta0 = beta0guess, beta1 = beta1guess) #fig = go.Figure() #fig = go.Scatter(x = np.arange(0,3500,1), y = beta0pathR, mode=&#39;lines&#39;) #fig.add_vline(x = result[0],line_dash = &#39;dash&#39;, line_width = 0.5, line_color = &quot;Blue&quot;) #fig.show() . plttitles = (&quot;Learning Rate = &quot; + str(learningRates[0]), &quot;Learning Rate = &quot; + str(learningRates[0]), &quot;Learning Rate = &quot; + str(learningRates[1]), &quot;Learning Rate = &quot; + str(learningRates[1]), &quot;Learning Rate = &quot; + str(learningRates[2]), &quot;Learning Rate = &quot; + str(learningRates[2])) fig = make_subplots(rows=3, cols=2, shared_xaxes = True, subplot_titles = plttitles) colors = [&#39;blue&#39;,&#39;green&#39;, &#39;gray&#39;] names = [&#39;Cold&#39;, &#39;Just Right&#39;, &#39;Hot&#39;] for i in [1,2]: for j in [1,2,3]: if i == 1: fig.add_trace(go.Scatter(x = np.arange(0,epochs,1), y = beta0results[j-1], mode=&#39;lines&#39; , marker_color = colors[j-1], name = names[j-1]+&#39; beta0&#39;), row = j, col = i) fig.add_hline(y = intercept,line_dash = &#39;dash&#39;, line_width = 1, line_color = &quot;red&quot;, row = j, col = i) fig.update_yaxes(range=[beta0guess, intercept + 0.5], row = j, col = i) else: fig.add_trace(go.Scatter(x = np.arange(0,epochs,1), y = beta1results[j-1], mode=&#39;lines&#39; , marker_color = colors[j-1], name = names[j-1]+&#39; beta1&#39;), row = j, col = i) fig.add_hline(y = slope,line_dash = &#39;dash&#39;, line_width = 1, line_color = &quot;red&quot;, row = j, col = i) fig.update_yaxes(range=[beta1guess, slope + 5], row = j, col = i) fig.update_layout(height = 1000, title_text=&quot;Impact of learning rate on gradient descent solution convergence&quot;) fig.show() . As seen in the graphs above, in the gradient descent model, the learning rate impacts how the algorithm converges to a solution. Of particular interest is the Just right vs the Hot scenario. In the Just Right scenario, the algorithm converges to a solution. However a minute increase of 0.0005 in the learning rate causes the gradient descent model to vary widely and not converge to a solution. . What I did not expect was the difference in convergence behaviour between $ beta_0$ and $ beta_1$. In the Cold scenario, $ beta_0$ does not converge, however $ beta_1$ conerges to a solution. . This concludes my initial exploration of regression. In subsequent notebooks, I will apply my new found knowledge to more practical problems. .",
            "url": "https://mindfire83.github.io/deepdive/jupyter/2021/05/11/Exploring-Regression.html",
            "relUrl": "/jupyter/2021/05/11/Exploring-Regression.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Average Sydney Suburb - Data processing in Pandas",
            "content": "The Average Sydney Suburbs - Nationality edition . In this notebook, I want to investigate the multicultural background of Sydney suburbs and compare that to Sydney&#39;s overall composition. I intend to do this via resampling and calculating the associated p-value for every suburb. This will highlight the suburbs which are close to Sydney&#39;s overall composition (hence average) and those that are not average. . My hypothesis (which also sounds rather obvious) is that the multicultural ccomposition of most of Sydney&#39;s suburbs is not random, however I am also curious to see the suburbs where I cannot prove this. . In this notebook I cleaned the top 6 nationality data for Sydney. In this notebook, we will use the cleaned data sets to explore Sydney&#39;s average suburbs . Part 1: Setting up the data . I have two datasets: . Sydney&#39;s total population split by the top 6 Countries of birth of person (CBOP) and others | Population of each suburb in Sydney split by the top 6 CBOP and others | . In this part, I want to setup the data so that I can get the &#39;Total Variation Distance&#39; of each Suburb&#39;s nationality composition vs Sydney&#39;s nationality composition . import pandas as pd import numpy as np %matplotlib inline import matplotlib.pyplot as plt import matplotlib.ticker as mtick import seaborn as sns sns.set_context(&#39;notebook&#39;) . #Reading the cleaned up data sets sydney_cbop = pd.read_csv(&#39;Sydney_CBOP_Clean.csv&#39;) suburb_cbop = pd.read_csv(&#39;Sydney_Suburb_CBOP_Clean.csv&#39;) . sydney_cbop.columns = [&#39;CBOP&#39;, &#39;Population&#39;] sydney_cbop.set_index(&#39;CBOP&#39;, inplace = True) sydney_cbop . Population . CBOP . England 151617 | . Australia 2752123 | . China (excludes SARs and Taiwan) 224682 | . India 130579 | . Vietnam 81041 | . New Zealand 86522 | . Others 1397432 | . #Lets start with exploring Sydney&#39;s overall population breakdown sydney_cbop.sort_values(&#39;Population&#39;,ascending = False, inplace=True) sydney_cbop.rename({&#39;China (excludes SARs and Taiwan)&#39;: &#39;China&#39;}, inplace = True) #Calculate percentage of each Nationality sydney_cbop[&#39;Percentage&#39;] = sydney_cbop.Population/ sydney_cbop.Population.sum() fig, ax = plt.subplots(figsize=(10, 7)) sns.barplot(x = sydney_cbop.Percentage * 100, y = sydney_cbop.index, data = sydney_cbop) ax.xaxis.set_major_formatter(mtick.PercentFormatter()) ax.set_title(&#39;Sydney Population Composition&#39;) plt.show() display(sydney_cbop) . Population Percentage . CBOP . Australia 2752123 | 0.570507 | . Others 1397432 | 0.289683 | . China 224682 | 0.046576 | . England 151617 | 0.031430 | . India 130579 | 0.027069 | . New Zealand 86522 | 0.017936 | . Vietnam 81041 | 0.016800 | . About 57% of the population in Sydney was born in Australia. . #Adding an ID column to the Sydney population dataframe. This will help ensure that I can align values correctly #when I combine thisdata with the suburb populations sydney_cbop[&#39;ID&#39;] = sydney_cbop.reset_index().index sydney_cbop . Population Percentage ID . CBOP . Australia 2752123 | 0.570507 | 0 | . Others 1397432 | 0.289683 | 1 | . China 224682 | 0.046576 | 2 | . England 151617 | 0.031430 | 3 | . India 130579 | 0.027069 | 4 | . New Zealand 86522 | 0.017936 | 5 | . Vietnam 81041 | 0.016800 | 6 | . #Lets now take a look at the Suburb nationality table suburb_cbop.describe() . Population . count 6251.000000 | . mean 771.674452 | . std 1849.102994 | . min -3.000000 | . 25% 11.000000 | . 50% 77.000000 | . 75% 474.000000 | . max 22907.000000 | . #We have a few cases where the Other population is &lt; 0. We will need to set it to zero suburb_cbop[&#39;Population&#39;] = np.where(suburb_cbop[&#39;Population&#39;] &lt; 0, 0, suburb_cbop[&#39;Population&#39;]) #Checking for &lt; 0 population. This should return an empty dataset suburb_cbop[suburb_cbop.Population &lt; 0].info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 0 entries Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 CBOP 0 non-null object 1 Suburb 0 non-null object 2 Population 0 non-null float64 dtypes: float64(1), object(2) memory usage: 0.0+ bytes . #Cleaning the data a bit and setting suburb as the index suburb_cbop.set_index(&#39;CBOP&#39;, inplace = True) suburb_cbop.rename({&#39;China (excludes SARs and Taiwan)&#39;: &#39;China&#39;}, inplace = True) suburb_cbop.reset_index(inplace = True) suburb_cbop.set_index(&#39;Suburb&#39;, inplace = True) suburb_cbop . CBOP Population . Suburb . Abbotsbury England | 40.0 | . Abbotsford (NSW) England | 181.0 | . Acacia Gardens England | 63.0 | . Agnes Banks England | 23.0 | . Airds England | 46.0 | . ... ... | ... | . Yellow Rock (Blue Mountains - NSW) Others | 94.0 | . Yennora Others | 751.0 | . Yerranderie Others | 3.0 | . Yowie Bay Others | 349.0 | . Zetland Others | 3454.0 | . 6251 rows × 2 columns . Next step is to get the composition for every suburb in Sydney. For this we will use Panda&#39;s groupby function with the transform function . _Note: Took me some time to figure this out. This was the best way that I found_ . suburb_cbop.reset_index(inplace = True) suburb_cbop[&#39;Percentage&#39;] = suburb_cbop[&#39;Population&#39;]/suburb_cbop.groupby(&#39;Suburb&#39;).Population.transform(&#39;sum&#39;) suburb_cbop . Suburb CBOP Population Percentage . 0 Abbotsbury | England | 40.0 | 0.009416 | . 1 Abbotsford (NSW) | England | 181.0 | 0.033674 | . 2 Acacia Gardens | England | 63.0 | 0.016570 | . 3 Agnes Banks | England | 23.0 | 0.025584 | . 4 Airds | England | 46.0 | 0.015726 | . ... ... | ... | ... | ... | . 6246 Yellow Rock (Blue Mountains - NSW) | Others | 94.0 | 0.090211 | . 6247 Yennora | Others | 751.0 | 0.465304 | . 6248 Yerranderie | Others | 3.0 | 1.000000 | . 6249 Yowie Bay | Others | 349.0 | 0.113570 | . 6250 Zetland | Others | 3454.0 | 0.342659 | . 6251 rows × 4 columns . #Setting suburb as index suburb_cbop.set_index(&#39;Suburb&#39;, inplace=True) suburb_cbop . CBOP Population Percentage . Suburb . Abbotsbury England | 40.0 | 0.009416 | . Abbotsford (NSW) England | 181.0 | 0.033674 | . Acacia Gardens England | 63.0 | 0.016570 | . Agnes Banks England | 23.0 | 0.025584 | . Airds England | 46.0 | 0.015726 | . ... ... | ... | ... | . Yellow Rock (Blue Mountains - NSW) Others | 94.0 | 0.090211 | . Yennora Others | 751.0 | 0.465304 | . Yerranderie Others | 3.0 | 1.000000 | . Yowie Bay Others | 349.0 | 0.113570 | . Zetland Others | 3454.0 | 0.342659 | . 6251 rows × 3 columns . Part 2: Hypothesis testing on population varience . In this part, I will: 1) Assume that overall Sydney&#39;s population composition would hold true for all suburbs 2) For each suburb, I will draw samples from Sydney&#39;s population (sample size will be equal to suburbs&#39;s total population) 3) Calculate the total variation distance (TVD) of the samples from Sydney&#39;s actual population composition. A total variation distance of 0 means that the nationality proportions are the same. Total variation distance is calculated using the absolute difference in population proportions which means that the minimum value is 0 4) Compare the results to the actual observed population variation . My null hypothesis is that the population of every suburb is completely due to chance. Using the 4 steps above for every suburb will help determine if this hypothesis is true . First, lets start with one suburb to see how this works . test = suburb_cbop.loc[&#39;Artarmon&#39;].join(sydney_cbop, on=&#39;CBOP&#39;, lsuffix=&quot;W&quot;) test[&#39;Var&#39;] = np.abs(test.PercentageW - test.Percentage) display(test.sort_values(by=&#39;ID&#39;)) #using display to print a pretty version of the data frame print(&quot;TVD between overall Sydney population and Artarmon population: &quot;, test.Var.sum()) print(&quot;Artarmon&#39;s population: &quot;, test.PopulationW.sum()) . CBOP PopulationW PercentageW Population Percentage ID Var . Suburb . Artarmon Australia | 4428.0 | 0.465077 | 2752123 | 0.570507 | 0 | 0.105430 | . Artarmon Others | 3153.0 | 0.331163 | 1397432 | 0.289683 | 1 | 0.041479 | . Artarmon China | 924.0 | 0.097049 | 224682 | 0.046576 | 2 | 0.050473 | . Artarmon England | 314.0 | 0.032980 | 151617 | 0.031430 | 3 | 0.001550 | . Artarmon India | 455.0 | 0.047789 | 130579 | 0.027069 | 4 | 0.020720 | . Artarmon New Zealand | 183.0 | 0.019221 | 86522 | 0.017936 | 5 | 0.001285 | . Artarmon Vietnam | 64.0 | 0.006722 | 81041 | 0.016800 | 6 | 0.010078 | . TVD between overall Sydney population and Artarmon population: 0.23101455275608412 Artarmon&#39;s population: 9521.0 . The current TVD of Artarmon&#39;s population compared to Sydney is 0.23. Next step would be to draw Artarmon&#39;s population sized samples based on Sydney&#39;s nationality breakdown. Then calculate the TVD of these samples and see how they compare to the actual TVD . #Extrating population percentages as numpy arrays for the multinomial distribution Syd_pop_perc = sydney_cbop.Percentage.to_numpy() Art_pop_perc = test.sort_values(by=&#39;ID&#39;).PercentageW.to_numpy() Art_pop = test.PopulationW.sum() #Generating 100000 multinomial samples of the size of Artarmon&#39;s population and Sydney&#39;s nationality breakdown #and dividing by Artarmon&#39;s population samples = (np.random.multinomial(Art_pop,Syd_pop_perc, size = 100000))/Art_pop #Calculating TVD variations = np.abs(samples - Syd_pop_perc).sum(axis=1) print(&quot;Average TVD: &quot;, np.average(variations)) print(&quot;Standard Dev: &quot;, np.std(variations)) variations.shape . Average TVD: 0.014365581508095708 Standard Dev: 0.005687543922900055 . (100000,) . The observed TVD of 0.23 is mush higher than the sample average TVD. Lets plot these results to get a better idea of the difference . fig, ax = plt.subplots(figsize=(10, 7)) sns.distplot(variations, bins=100, kde=False, label=&#39;Var Dist&#39;) ax.set_xlabel(&quot;TVD&quot;,fontsize=16) #ax.set_ylabel(&quot;Frequency&quot;,fontsize=16) plt.axvline(test.Var.sum(), color=&#39;red&#39;, label=&#39;Observed TVD&#39;) plt.legend() plt.tight_layout() plt.show() . Based on the graph, it is clear that Artarmon&#39;s population breakdown is not due to a random placement. Lets now scale this process up for all of Sydney&#39;s suburbs (with a population size &gt; 1000) First I will create a dataframe with Suburb, total population, observed and sample TVD. Then I will write a function to calculate and fill in the observed and sample TVD for all suburbs . suburb_TVD = suburb_cbop.groupby([&#39;Suburb&#39;]).sum() #Excluding suburbs with population &lt; 1000 suburb_TVD = suburb_TVD[suburb_TVD.Population &gt;= 1000].copy() #Add observed and sample TVD column and set to 0 suburb_TVD[&#39;observed_TVD&#39;] = 0 suburb_TVD[&#39;sample_TVD&#39;] = 0 suburb_TVD[&#39;sample_STD&#39;] = 0 suburb_TVD.drop([&#39;Percentage&#39;], inplace=True, axis = 1) suburb_TVD . Population observed_TVD sample_TVD sample_STD . Suburb . Abbotsbury 4248.0 | 0 | 0 | 0 | . Abbotsford (NSW) 5375.0 | 0 | 0 | 0 | . Acacia Gardens 3802.0 | 0 | 0 | 0 | . Airds 2925.0 | 0 | 0 | 0 | . Alexandria 8253.0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | . Yarrawarrah 2731.0 | 0 | 0 | 0 | . Yellow Rock (Blue Mountains - NSW) 1042.0 | 0 | 0 | 0 | . Yennora 1614.0 | 0 | 0 | 0 | . Yowie Bay 3073.0 | 0 | 0 | 0 | . Zetland 10080.0 | 0 | 0 | 0 | . 674 rows × 4 columns . #There are probably faster ways to do this, but its late, im tired and I want to hit my target of the day #So ill create a function to take in each suburb and calcualte the observed TVD and sample TVD of 100000 trials def calculate_TVD(suburb_df, Syd_pop_perc = Syd_pop_perc): Sub_pop_perc = suburb_df.sort_values(by=&#39;ID&#39;).PercentageW.to_numpy() Sub_pop = suburb_df.PopulationW.sum() samples = (np.random.multinomial(Sub_pop,Syd_pop_perc, size = 100000))/Sub_pop #Calculating TVD variations = np.abs(samples - Syd_pop_perc).sum(axis=1) #returning observed and sample TVD and the sample standard deviation return np.abs(Sub_pop_perc - Syd_pop_perc).sum(), np.average(variations), np.std(variations) . #Testing the function with Artarmon print(calculate_TVD(suburb_cbop.loc[&#39;Artarmon&#39;].join(sydney_cbop, on=&#39;CBOP&#39;, lsuffix=&quot;W&quot;))) . (0.23101455275608415, 0.014353218703339833, 0.005699959579108245) . #Next step is to call this function for all suburbs in the suburb_TVD table #and then fill in the actual and sample TVD values for suburb in suburb_TVD.index: result = calculate_TVD(suburb_cbop.loc[suburb].join(sydney_cbop, on=&#39;CBOP&#39;, lsuffix=&quot;W&quot;)) suburb_TVD.loc[suburb, &#39;observed_TVD&#39;] = result[0] suburb_TVD.loc[suburb, &#39;sample_TVD&#39;] = result[1] suburb_TVD.loc[suburb, &#39;sample_STD&#39;] = result[2] display(suburb_TVD) . Population observed_TVD sample_TVD sample_STD . Suburb . Abbotsbury 4248.0 | 0.189052 | 0.021538 | 0.008537 | . Abbotsford (NSW) 5375.0 | 0.134545 | 0.019082 | 0.007546 | . Acacia Gardens 3802.0 | 0.177846 | 0.022760 | 0.009045 | . Airds 2925.0 | 0.295935 | 0.025932 | 0.010273 | . Alexandria 8253.0 | 0.116368 | 0.015450 | 0.006120 | . ... ... | ... | ... | ... | . Yarrawarrah 2731.0 | 0.590341 | 0.026858 | 0.010650 | . Yellow Rock (Blue Mountains - NSW) 1042.0 | 0.583075 | 0.043546 | 0.017256 | . Yennora 1614.0 | 0.479971 | 0.034902 | 0.013820 | . Yowie Bay 3073.0 | 0.525761 | 0.025265 | 0.010013 | . Zetland 10080.0 | 0.638076 | 0.013930 | 0.005505 | . 674 rows × 4 columns . #Calcualating deviations from mean for all suburbs suburb_TVD[&#39;devs_from_mean&#39;] = (suburb_TVD.observed_TVD - suburb_TVD.sample_TVD)/suburb_TVD.sample_STD suburb_TVD.describe() . Population observed_TVD sample_TVD sample_STD devs_from_mean . count 674.000000 | 674.000000 | 674.000000 | 674.000000 | 674.000000 | . mean 7038.764095 | 0.336198 | 0.021271 | 0.008422 | 43.871629 | . std 6213.960783 | 0.165464 | 0.008553 | 0.003385 | 32.473338 | . min 1003.000000 | 0.044193 | 0.006450 | 0.002564 | 2.406396 | . 25% 2847.000000 | 0.198069 | 0.014487 | 0.005733 | 22.075065 | . 50% 5098.000000 | 0.313934 | 0.019629 | 0.007766 | 35.218748 | . 75% 9389.000000 | 0.477288 | 0.026286 | 0.010405 | 55.409502 | . max 47173.000000 | 1.057948 | 0.044156 | 0.017531 | 218.335893 | . display(suburb_TVD.sort_values(by=&#39;devs_from_mean&#39;)) . Population observed_TVD sample_TVD sample_STD devs_from_mean . Suburb . Mortlake (NSW) 1062.0 | 0.084012 | 0.043002 | 0.017042 | 2.406396 | . Point Piper 1414.0 | 0.094179 | 0.037289 | 0.014770 | 3.851866 | . Summer Hill (Inner West - NSW) 7303.0 | 0.044193 | 0.016425 | 0.006502 | 4.270783 | . Canada Bay 1246.0 | 0.107523 | 0.039757 | 0.015824 | 4.282415 | . North Willoughby 4176.0 | 0.061874 | 0.021679 | 0.008590 | 4.679585 | . ... ... | ... | ... | ... | ... | . Liverpool 27090.0 | 0.637732 | 0.008523 | 0.003364 | 187.015285 | . Sydney 17249.0 | 0.835058 | 0.010656 | 0.004209 | 195.874182 | . Parramatta 25798.0 | 0.731907 | 0.008734 | 0.003467 | 208.612084 | . Hurstville 29832.0 | 0.692458 | 0.008111 | 0.003212 | 213.034828 | . Auburn (NSW) 37372.0 | 0.632927 | 0.007241 | 0.002866 | 218.335893 | . 674 rows × 5 columns . From the calculation, it seems that most suburbs do not have a truly random population breakdown. Next, I will try to investigate this a bit more. Starting with a scatter plot of population and observed TVD . fig, ax = plt.subplots(figsize=(10, 7)) sns.scatterplot(data=suburb_TVD, y=&quot;Population&quot;, x=&quot;observed_TVD&quot;) ax.set_title(&#39;Observed TVD vs Suburb population&#39;) plt.tight_layout() plt.show() . I can observe no clear trend here. Intutively these results makes sense. I can highlight the the following reasons: . There are no policies in place in Sydney to manage population compositions in locations | This is not a truly independent distribution as I have assumed for eg: it is highly likely that 1 person of a particular nationality is married to a person of the same nationality | Anecdotally, a clustering behaviour is observed in Sydeny (eg: Vietnames concentration on Cabramatta). Th | . Total suburb population numbers point to a clusterng behaviour for groups of people from the same country/back g .",
            "url": "https://mindfire83.github.io/deepdive/jupyter/2021/05/11/Average-Sydney-Suburb-Nationality.html",
            "relUrl": "/jupyter/2021/05/11/Average-Sydney-Suburb-Nationality.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Average Sydney Suburb - Data munging in Pandas",
            "content": "The Average Sydney Suburb - Data cleaning . I have recently finished an eDx course on inferential statistics from Duke university. This course does a good job of using resampling for inferential statistics. . Applying what I have learned in this course, I want to figure out the most average Sydeny suburbs in terms of the country of birth of person (CBOP) and age. . There is a lot of data available via ABS. I used the Census table builder to get the CBOP and age data. . In this notebook, I will clean and setup the data for my analysis . #Importing the libraries that I will use import pandas as pd import numpy as np %matplotlib inline from IPython.display import display . I have downloaded the following files from the 2016 Census that I will need to use for my analysis: . Population Total Sydney population | Top 6 CBOP population for Sydney | Total population for all of Sydney suburbs | Top 6 CBOP population for all of Sydney suburbs | . | Age Sydney&#39;s age in single digits | Sydney&#39;s suburb&#39;s age in single digits | . | . As useful as the Census table builder is, I could not get the data in the format that I needed - hence the need for this cleaning exercise. In the past I have made use of excel and pandas for cleaning data, however in this notebook I will attempt to clean all the data in pandas. . Starting with Sydney&#39;s CBOP mix. My aim is to create a table with the top 6 CBOP and Others. . #Read Sydney pop and top 6 cbop sydney_pop = pd.read_csv(&#39;greater_sydney_pop.csv&#39;, skiprows = 11) sydney_cbop = pd.read_csv(&#39;top_6_cobp.csv&#39;, skiprows = 11) . For the Sydney population table, all I need is the total population value. I can get this from Index 0 and column 4823993 . syd_pop = int(sydney_pop.at[0,&#39;4823993&#39;]) print(&quot;Sydney&#39;s population: &quot;, syd_pop) . Sydney&#39;s population: 4823993 . Now lets take a look at the Sydney CBOP table . #Using threshold values in drop na to only drop rows with 2 Na values. #This has the useful side effect of dropping the unnamed column as well sydney_cbop.dropna(axis = 1, thresh=2, inplace = True) sydney_cbop.dropna(axis = 0, inplace = True) #rename BPLP - 4 Digit Level column to Population sydney_cbop.rename(columns = {sydney_cbop.columns[0]:&#39;Population&#39;}, inplace = True) #Drop info index sydney_cbop.drop(&#39;INFO&#39;, inplace = True) #convert population numbers to integer sydney_cbop[&#39;Population&#39;] = sydney_cbop[&#39;Population&#39;].apply(pd.to_numeric) #Rename the total index value to CBOP Total. This will be used later to calculate the value of other countries sydney_cbop.rename({&#39;Total&#39;: &#39;CBOP Total&#39;}, inplace = True) sydney_cbop.info() print(sydney_cbop) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 7 entries, England to CBOP Total Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 Population 7 non-null int64 dtypes: int64(1) memory usage: 112.0+ bytes Population England 151617 Australia 2752123 China (excludes SARs and Taiwan) 224682 India 130579 Vietnam 81041 New Zealand 86522 CBOP Total 3426561 . Next step is to add the population of people who are not from any of the 6 top countries . sydney_cbop.loc[&#39;CBOP Total&#39;] = syd_pop - sydney_cbop.at[&#39;CBOP Total&#39;, &#39;Population&#39;] sydney_cbop.rename({&#39;CBOP Total&#39; : &#39;Others&#39;}, inplace = True) sydney_cbop.Population.sum() . 4823996 . print(sydney_cbop) print(&#39; nTotal Sydney Population &#39;, sydney_cbop.Population.sum()) . Population England 151617 Australia 2752123 China (excludes SARs and Taiwan) 224682 India 130579 Vietnam 81041 New Zealand 86522 Others 1397432 Total Sydney Population 4823996 . All set!, we now have a table with the top 6 natonalities (CBOP) of Sydney and an others column. Next step is to repeat this process for every suburb in the greater Sydney area. . Note: It took me about 2 hours to figure out the best way to clean these two small dataframes and boil it down to a few lines of code. I am putting this here as a reminder to not get overwhelmed when I see posts where things like these seem to be the simplest of steps. Having gone through this exercise, its clear that data cleaning is very important and not as easy as it seems. Atleast when you are just getting started. Keep at it!! . #Load the top 6 CBOP by Suburb and total population by suburb files suburb_cbop = pd.read_csv(&#39;suburb_top_6_cobp.csv&#39;, skiprows = 11) suburb_pop = pd.read_csv(&#39;total_suburb_population.csv&#39;, skiprows = 11) . Additional challenges in this dataset: 1) Index needs to be reset and forward filled to remove the NAN values from the index 2) When extracting the data from ABS, although I filtered on Sydney, it still gave me a list of all suburbs. The difference here is that for suburbs that are not in the Greater Sydney area, the population is 0. This should be easier to remove . #Reset Index suburb_cbop.reset_index(inplace = True) #Forward fill the country name and copy only the country column. We need the NaNs for cleaning purposes suburb_cbop[&#39;index&#39;] = suburb_cbop.ffill()[&#39;index&#39;] suburb_cbop . index BPLP - 4 Digit Level SSC (UR) Unnamed: 2 . 0 England | Aarons Pass | 0.0 | NaN | . 1 England | Abbotsbury | 40.0 | NaN | . 2 England | Abbotsford (NSW) | 181.0 | NaN | . 3 England | Abercrombie | 0.0 | NaN | . 4 England | Abercrombie River | 0.0 | NaN | . ... ... | ... | ... | ... | . 31681 Total | Migratory - Offshore - Shipping (NSW) | 0.0 | NaN | . 31682 Data Source: Census of Population and Housing,... | NaN | NaN | NaN | . 31683 INFO | Cells in this table have been randomly adjuste... | NaN | NaN | . 31684 Copyright Commonwealth of Australia, 2018, see... | NaN | NaN | NaN | . 31685 ABS data licensed under Creative Commons, see ... | NaN | NaN | NaN | . 31686 rows × 4 columns . #Set index back to country suburb_cbop.set_index(&#39;index&#39;, inplace = True) suburb_cbop.index.rename(&#39;CBOP&#39;, inplace = True) . #Removing rows with 2 NaN values suburb_cbop.dropna(axis = 1, thresh = 2, inplace = True) suburb_cbop.dropna(axis = 0, thresh = 2, inplace = True) col_names = [&#39;Suburb&#39;, &#39;Population&#39;] suburb_cbop.columns = col_names suburb_cbop.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 31682 entries, England to Total Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 Suburb 31682 non-null object 1 Population 31682 non-null float64 dtypes: float64(1), object(1) memory usage: 742.5+ KB . Now that we have removed the NaN values, lets look at the numbers and clean those up as well. But it is not as simple as it seems. There could be suburbs in Sydney which do not have any person from one of the top 6 nationalities. In that case, I want to preserve the 0 associated with that particular nationality for my analysis. So before I start cleaning up the numbers, lets first clean up the total suburb population dataframe . suburb_pop.dropna(axis = 1, thresh = 2, inplace = True) suburb_pop.dropna(axis = 0, thresh = 1, inplace = True) #Drop info column suburb_pop.drop(&#39;INFO&#39;, inplace = True) suburb_pop.columns = [&#39;Population&#39;] #Convert population to integer suburb_pop[&#39;Population&#39;] = suburb_pop[&#39;Population&#39;].apply(pd.to_numeric) suburb_pop.rename_axis(&#39;Suburb&#39;, inplace = True) print(suburb_pop.info()) print(&quot;Sydney Population: &quot;, suburb_pop.Population.sum()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 4526 entries, Aarons Pass to Migratory - Offshore - Shipping (NSW) Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 Population 4526 non-null int64 dtypes: int64(1) memory usage: 70.7+ KB None Sydney Population: 4823982 . The suburb_pop table now has the population of every suburb in Greater Sydney. For the remaining suburbs the population would be zero. So if we drop the 0 populations we should be left with Sydney suburbs only . #Get index names of suburbs with zero populations zero_suburbs = suburb_pop[suburb_pop.Population == 0].index #Drop these index names suburb_pop.drop(zero_suburbs, inplace=True) suburb_pop.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 893 entries, Abbotsbury to Zetland Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 Population 893 non-null int64 dtypes: int64(1) memory usage: 14.0+ KB . Dropping the 0 population suburbs leaves us with 893 suburbs. I can also use the same index names to update the top nationalities data frame (suburb_cbop). For that I would first need to set the suburb as the index and then drop . suburb_cbop.reset_index(inplace = True) suburb_cbop.set_index(&#39;Suburb&#39;, inplace = True) suburb_cbop.loc[()] . CBOP Population . Suburb . Aarons Pass England | 0.0 | . Abbotsbury England | 40.0 | . Abbotsford (NSW) England | 181.0 | . Abercrombie England | 0.0 | . Abercrombie River England | 0.0 | . ... ... | ... | . Yuraygir Total | 0.0 | . Zara Total | 0.0 | . Zetland Total | 6618.0 | . No usual address (NSW) Total | 0.0 | . Migratory - Offshore - Shipping (NSW) Total | 0.0 | . 31682 rows × 2 columns . suburb_cbop.drop(zero_suburbs, inplace = True) suburb_cbop.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 6251 entries, Abbotsbury to Zetland Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 CBOP 6251 non-null object 1 Population 6251 non-null float64 dtypes: float64(1), object(1) memory usage: 146.5+ KB . suburb_cbop.reset_index(inplace = True) suburb_cbop.set_index([&#39;CBOP&#39;,&#39;Suburb&#39;], inplace = True) suburb_cbop . Population . CBOP Suburb . England Abbotsbury 40.0 | . Abbotsford (NSW) 181.0 | . Acacia Gardens 63.0 | . Agnes Banks 23.0 | . Airds 46.0 | . ... ... ... | . Total Yellow Rock (Blue Mountains - NSW) 941.0 | . Yennora 868.0 | . Yerranderie 3.0 | . Yowie Bay 2725.0 | . Zetland 6618.0 | . 6251 rows × 1 columns . print(suburb_cbop.loc[&#39;Total&#39;, &#39;Zetland&#39;]) print(suburb_pop.loc[&#39;Zetland&#39;]) suburb_cbop.loc() . Population 6618.0 Name: (Total, Zetland), dtype: float64 Population 10072 Name: Zetland, dtype: int64 . &lt;pandas.core.indexing._LocIndexer at 0x11af9f630&gt; . Finally we are now in a position to create an others row for each suburb in Sydney. The others row will contain the number of people who are not from the top 6 countries . #Joining the suburb top 6 with suburb totap population dataframes and extracting the &#39;Total&#39; index only others_temp = suburb_cbop.join(suburb_pop, on=&#39;Suburb&#39;, rsuffix = &quot;r&quot;).loc[(&#39;Total&#39;, slice(None))] #Calculating the others population value others_temp.Population = others_temp.Populationr - others_temp.Population others_temp.drop([&#39;Populationr&#39;], axis=1, inplace = True) #Creating a final dataframe with others population value in Population_r column suburb_cbop_final = suburb_cbop.join(others_temp, on=&#39;Suburb&#39;, rsuffix = &#39;_r&#39;) #Setting the Total index value to Nan suburb_cbop_final.loc[(&#39;Total&#39;, slice(None)),&#39;Population&#39;] = np.nan #renaming index value Total to Others suburb_cbop_final.rename(index={&#39;Total&#39;:&#39;Others&#39;}, inplace = True) #Setting the right population value in the Others index suburb_cbop_final[&#39;Population&#39;] = np.where(suburb_cbop_final[&#39;Population&#39;].isnull(), suburb_cbop_final[&#39;Population_r&#39;], suburb_cbop_final[&#39;Population&#39;]) suburb_cbop_final.drop([&#39;Population_r&#39;], axis=1, inplace = True) suburb_cbop_final . Population . CBOP Suburb . England Abbotsbury 40.0 | . Abbotsford (NSW) 181.0 | . Acacia Gardens 63.0 | . Agnes Banks 23.0 | . Airds 46.0 | . ... ... ... | . Others Yellow Rock (Blue Mountains - NSW) 94.0 | . Yennora 751.0 | . Yerranderie 3.0 | . Yowie Bay 349.0 | . Zetland 3454.0 | . 6251 rows × 1 columns . That took a lot of steps and although I have setup what I wanted to, I refuse to believe that I cant find a more efficient way to do this calculation. Lets try this again . #resetting the index in the suburb cbop file suburb_cbop.reset_index(inplace = True) suburb_cbop.set_index(&#39;Suburb&#39;, inplace=True) suburb_cbop_final = suburb_cbop.join(suburb_pop, on=&#39;Suburb&#39;, rsuffix = &quot;r&quot;) suburb_cbop_final[&#39;Population&#39;] = np.where(suburb_cbop_final[&#39;CBOP&#39;] == &#39;Total&#39;, suburb_cbop_final[&#39;Populationr&#39;] - suburb_cbop_final[&#39;Population&#39;], suburb_cbop_final[&#39;Population&#39;]) suburb_cbop_final.drop([&#39;Populationr&#39;], axis=1, inplace = True) suburb_cbop_final . CBOP Population . Suburb . Abbotsbury England | 40.0 | . Abbotsford (NSW) England | 181.0 | . Acacia Gardens England | 63.0 | . Agnes Banks England | 23.0 | . Airds England | 46.0 | . ... ... | ... | . Yellow Rock (Blue Mountains - NSW) Total | 94.0 | . Yennora Total | 751.0 | . Yerranderie Total | 3.0 | . Yowie Bay Total | 349.0 | . Zetland Total | 3454.0 | . 6251 rows × 2 columns . #As a final step, renaming Total to Others suburb_cbop_final.reset_index(inplace=True) suburb_cbop_final.set_index(&#39;CBOP&#39;, inplace = True) suburb_cbop_final.rename({&#39;Total&#39;: &#39;Others&#39;}, inplace = True) . That was simpler. Not I will save my clean files for part two. . suburb_cbop_final . Suburb Population . CBOP . England Abbotsbury | 40.0 | . England Abbotsford (NSW) | 181.0 | . England Acacia Gardens | 63.0 | . England Agnes Banks | 23.0 | . England Airds | 46.0 | . ... ... | ... | . Others Yellow Rock (Blue Mountains - NSW) | 94.0 | . Others Yennora | 751.0 | . Others Yerranderie | 3.0 | . Others Yowie Bay | 349.0 | . Others Zetland | 3454.0 | . 6251 rows × 2 columns . sydney_cbop.to_csv(&#39;Sydney_CBOP_Clean.csv&#39;) suburb_cbop_final.to_csv(&#39;Sydney_Suburb_CBOP_Clean.csv&#39;) .",
            "url": "https://mindfire83.github.io/deepdive/jupyter/2021/05/11/Average-Sydney-Suburb-Data-Cleaning.html",
            "relUrl": "/jupyter/2021/05/11/Average-Sydney-Suburb-Data-Cleaning.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploring Monty Hall in python",
            "content": "Getting Started . I have started a course on probability from MIT Open courseware . In an attempt to solidify my learning and also get started on my blog I will try to code interesting problems/topics that I come across. . Starting with the Monty Hall problem. . In this notebook, I will attempt to: . Create a Simulation of a single trial of the Monty Hall Problem (with the option to switch) | Run the simulation multile number of times (with switching, without switching and random switchnig) and explore the results | Figure out if I can make the simulation faster | . To get started, lets define a function that takes in three inputs (Prize door #, Selected door #, Switch) and returns the result of the experiment (win or loss). . Additionally in this case, a win with switching is a loss without switching (and vice versa) . # Returns of the trial results in a win def MontyTrial(win_door, selected_door, switch = False): if (win_door == selected_door and not switch): return 1 elif (win_door == selected_door and switch): return 0 elif (win_door != selected_door and switch): return 1 elif (win_door != selected_door and not switch): return 0 . Simulating the trials . Now that we have a function that can give us the outcome of an individual trial, lets see if we can run multiple trials and find out what happens. . I will use a Numpy array to generate random winning door numbers and selected door numbers. . Having set that up, I will now generate two arrays of size 1000 each. One would be the winning door number and the other would be the selected door number. . Then we can feed in the results to the MontyTrial function and evaluate the results . Edit: Going to convert this into a function so that I can time the results . #This function runs Monty Hall trials for the given number of trials. #The supress variable will stop any output from the function for timing purposes def MontySimulation(trials, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials&quot;) Winning_Door = np.random.randint(1,4,trials) Selected_Door = np.random.randint(1,4,trials) Result = 0 #Running the trial where the contestant never switches for i in range(0, len(Winning_Door)): Result += MontyTrial(Winning_Door[i], Selected_Door[i], False) Result = Result/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Result) print(&quot;Probability of winning with always switching: &quot;, 1 - Result) MontySimulation(100000) . Simulating 100000 trials Probability of winning with no switching: 0.3345 Probability of winning with always switching: 0.6655 . Looking at the results above, the probability of winning is ~67% or 2/3rd of the time if an always switching strategy is adopted. . However I think this implementation is not optimal and there is an opportunity to speed it up. To start, I will time 1 million trials. . %timeit MontySimulation(1000000, True) . 1.03 s ± 442 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . With the current logic, it takes about 1 second to run 1 million MontyHall trials. There is obviously room for improvement. To start, lets see if we can simplyfy the Monty Trial function . #In the v2 version of this function, I will try to reduce the for conditionals and run a trial to see if that improves performance def MontyTrialv2(win_door, selected_door, switch = False): result = 0 if win_door == selected_door else 1 return result == switch . Came down to a bit of a wierd logic, but this is what the function is doing: . If Winning door and selected door are the same then set result to 0, else set it to 1 | If there is no switching, then the variable switch is false (0) and so result would be equal to switch (and a win) | If result is one and no switching, then the return statement returns as false (1 == 0) | With switching (variable switch set to true) a zero resutls returns a false (since 0 != 1) | Finally, with switching set to true a 1 results returns True (since 1 == 1) | Lets plug this new function into a simulator and see if this speeds up the code . #V2 simulation fucntion uses the Monty V2 to generate results def MontySimulationv2(trials, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials in v2&quot;) Winning_Door = np.random.randint(1,4,trials) Selected_Door = np.random.randint(1,4,trials) Result = 0 #Running the trial where the contestant never switches for i in range(0, len(Winning_Door)): Result += MontyTrialv2(Winning_Door[i], Selected_Door[i], False) Result = Result/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Result) print(&quot;Probability of winning with always switching: &quot;, 1 - Result) MontySimulation(100000) MontySimulationv2(100000) . Simulating 100000 trials Probability of winning with no switching: 0.33616 Probability of winning with always switching: 0.66384 Simulating 100000 trials in v2 Probability of winning with no switching: 0.33048 Probability of winning with always switching: 0.66952 . print(&quot;Timing v1 Simulation on 1 million trials&quot;) %timeit MontySimulation(1000000, True) print(&quot;Timing v2 Simulation on 1 million trials&quot;) %timeit MontySimulationv2(1000000, True) . Timing v1 Simulation on 1 million trials 768 ms ± 81.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Timing v2 Simulation on 1 million trials 589 ms ± 4.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Not a measurable impact. It could be the different arrays generated in each function (unlikely!!) so lets just time the main trail function itself . print(&quot;Timing v1 trial function&quot;) %timeit MontyTrial(1,2,True) print(&quot;Timing v2 trial function&quot;) %timeit MontyTrialv2(1,2,True) . Timing v1 trial function 202 ns ± 10.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) Timing v2 trial function 210 ns ± 60.2 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) . Marginal impact. So lets try a completely different approach. Building this simulator, i have realised: . There is a mathematical function that can return the result of the simulation (thus there will ne no need for conditionals) | A win in the non-switching trial would be a loss in the switching trial (and vice versa) since there are only two doors left to choose from | . So using Numpy there could be a faster implementation. . #Numpy based implementation def MontySimulationv3(trials, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials in v3 (Numpy Version)&quot;) #Generate a winning and a trial array and compare. A true in this array results in a win in case of no switching Result = np.random.randint(1,4,trials) == np.random.randint(1,4,trials) #If Winning door and the selected doors are the same and there is no switching, then its a win Noswitch_win = Result.sum()/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Noswitch_win) #Winning with switching is 1 - probability of winning without switching print(&quot;Probability of winning with switching: &quot;, 1 - Noswitch_win) . MontySimulationv3(1000000) . Simulating 1000000 trials in v3 (Numpy Version) Probability of winning with no switching: 0.334154 Probability of winning with switching: 0.6658459999999999 . The final version of the simulator uses Numpy only to run trials. All logic is boiled down to a simple comparison of the winning door array and selected door array. Lets find out if that speeds up the model . print(&quot;Timing v1 Simulation on 1 million trials&quot;) %timeit MontySimulation(1000000, True) print(&quot;Timing v2 Simulation on 1 million trials&quot;) %timeit MontySimulationv2(1000000, True) print(&quot;Timing v3 Simulation on 1 million trials&quot;) %timeit MontySimulationv3(1000000, True) . Timing v1 Simulation on 1 million trials 687 ms ± 15.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Timing v2 Simulation on 1 million trials 638 ms ± 175 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Timing v3 Simulation on 1 million trials 27 ms ± 1.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . Woah...A Numpy only implementation is almost 10 times faster than my v1 and v2 versions, although all three versions generate their own arrays. Lets modify all three function to accept external arrays so that we can compare the results as well . trials = 1000000 Winning_Door = np.random.randint(1,4,trials) Selected_Door = np.random.randint(1,4,trials) def MontySimulation(trials,Winning_Door,Selected_Door, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials&quot;) Result = 0 #Running the trial where the contestant never switches for i in range(0, len(Winning_Door)): Result += MontyTrial(Winning_Door[i], Selected_Door[i], False) Result = Result/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Result) print(&quot;Probability of winning with always switching: &quot;, 1 - Result) def MontySimulationv2(trials,Winning_Door,Selected_Door, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials in v2&quot;) Result = 0 #Running the trial where the contestant never switches for i in range(0, len(Winning_Door)): Result += MontyTrialv2(Winning_Door[i], Selected_Door[i], False) Result = Result/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Result) print(&quot;Probability of winning with always switching: &quot;, 1 - Result) def MontySimulationv3(trials,Winning_Door,Selected_Door, supress=False): if (supress == False): print(&quot;Simulating &quot;,trials,&quot; trials in v3 (Numpy Version)&quot;) #Generate a winning and a trial array and compare. A true in this array results in a win in case of no switching Result = Winning_Door == Selected_Door #If Winning door and the selected doors are the same and there is no switching, then its a win Noswitch_win = Result.sum()/trials if (supress == False): print(&quot;Probability of winning with no switching: &quot;, Noswitch_win) #Winning with switching is 1 - probability of winning without switching print(&quot;Probability of winning with switching: &quot;, 1 - Noswitch_win) . MontySimulation(trials, Winning_Door, Selected_Door) MontySimulationv2(trials, Winning_Door, Selected_Door) MontySimulationv3(trials, Winning_Door, Selected_Door) . Simulating 1000000 trials Probability of winning with no switching: 0.333761 Probability of winning with always switching: 0.666239 Simulating 1000000 trials in v2 Probability of winning with no switching: 0.333761 Probability of winning with always switching: 0.666239 Simulating 1000000 trials in v3 (Numpy Version) Probability of winning with no switching: 0.333761 Probability of winning with switching: 0.666239 . Results from all three versions are the same. Lets do a final timing run for 1 million trials . trials = 1000000 Winning_Door = np.random.randint(1,4,trials) Selected_Door = np.random.randint(1,4,trials) print(&quot;Timing v1 Simulation on 1 million trials&quot;) %timeit MontySimulation(trials, Winning_Door, Selected_Door, True) print(&quot;Timing v2 Simulation on 1 million trials&quot;) %timeit MontySimulationv2(trials, Winning_Door, Selected_Door, True) print(&quot;Timing v3 Simulation on 1 million trials&quot;) %timeit MontySimulationv3(trials, Winning_Door, Selected_Door, True) . Timing v1 Simulation on 1 million trials 643 ms ± 7.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Timing v2 Simulation on 1 million trials 555 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Timing v3 Simulation on 1 million trials 2.04 ms ± 151 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . The Numpy implementation is now even faster pointing to the fact that generating the random array was the most time consuming part in the v3 algorithm. . Rounding Up . This notebook was my first attempt at implementing a problem and documenting the findings. Along the way I realsied there was further opportunity for me to optimize my code by optimizing my logic and also using faster tools (Numpy) . About the monty hall problem: . It is a well documented problem hence the results were not surprising | This is a significant deviation from the mean with a smaller number of trials, however the values converge for higher number of trials (which is to be expected) | .",
            "url": "https://mindfire83.github.io/deepdive/jupyter/2021/01/21/Exploring-Probability-in-python.html",
            "relUrl": "/jupyter/2021/01/21/Exploring-Probability-in-python.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Exploring Poisson in python",
            "content": "Getting started with Poisson . I have finally reached the Poisson process lectures in this course . At the starting point it seems a little overwhelming as I am confused between the poisson process, its relation to the exponential process and interarrival times. So to get a gentle start in this, I will start with a basic exploration of the poisson distribution and the interarrival times. . I will start by following this post from Towards Data Science. This post comes with a jupyter notebook, however I will try to create the output without referring to the notebook (or state where I did refer to it) . To start, . The Poisson Distribution probability mass function gives the probability of observing k events in a time period given the length of the period and the average events per time . given by the formula: begin{align*} mathbf{P}( mathbf{k} text{ events in interval}) = e^{- lambda} frac{ lambda^k}{k!} end{align*} . I will start with visualising the same of this distribution . import numpy as np from scipy.stats import poisson import scipy.stats as stats import seaborn as sns import matplotlib.pyplot as plt import math . #Lets plot some PMFs sns.set(style=&quot;darkgrid&quot;) sns.set_context(&quot;notebook&quot;, font_scale=1.5, rc={&quot;lines.linewidth&quot;: 2.5}) #setting line width fig, ax = plt.subplots(figsize=(18, 8)) for i in range(1,7): po = poisson(i) sns.lineplot(x = range(0,15),y = [po.pmf(x) for x in range(0,15)], label = &quot;$ lambda$ = {}&quot;.format(i)) . For now I cant seem to get markers working in seaborn. Will update when I figure out a solution to that . However, more importantly, the graphs line up with the lambda value highlighting the highest probability for events which are close to the value of lambda . Moving on, lets generate a large number of poison rvs and and compare it to the PMF . randObj = np.random.RandomState(443325) i = 5 x, y = np.unique(randObj.poisson(i,2000), return_counts = True) y = y/y.sum() po = poisson(i) fig, ax = plt.subplots(figsize=(18, 8)) #sns.distplot(, kde=False) sns.lineplot(x = x,y = y,dashes=False, label = &quot;RVs (Numpy)&quot;, markers = True) sns.lineplot(x = range(0,15),y = [po.pmf(x) for x in range(0,15)], label = &quot;$ lambda$ = {}&quot;.format(i)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc3f678be50&gt; . I ended up generating 2000 poisson rvs to show a slight deviation to the poisson PMF. This wraps up my first exploration into the poisson PMF. . Now lets simulate $ lambda$ of 6 for 10,000 obesrvations and plot a histogram of the resutls . fig, ax = plt.subplots(figsize=(18, 8)) lam = 6 sns.distplot(randObj.poisson(lam,10000), kde=False, label=&quot;Poisson RVs for $ lambda = {}&quot;.format(lam)) plt.legend() plt.show() . The distribution of events looks roughly normal centered around the rate of 6 which lines up with our understanding. . Now onto the part that I am having a bit of a hard time with - interarrival times. The intervarrival times for a poisson distribution is given by an exponential distribution with parameter $ lambda$ . For a $ lambda$ of 6/hour, it would imply 1 arrival every 10 minutes. . From the exponential distribution the probabilitity of waiting more than time t is given by: . begin{align*} mathbf{P}( text{ T &gt; t}) = e^{- frac{ text{events}}{time}*t} end{align*}With our $ lambda$ of 6, this equation becomes: . begin{align*} mathbf{P}( text{ T &gt; t}) = e^{- frac{ text{6}}{60}*t} end{align*}Note: 1) Hoping this helps someone. Scipy&#39;s impementation of the exponential distribution uses $ frac{1}{ beta}$ as the rate parameter instead of $ lambda$ additionally, dont get confused with PDF and CDF. In the case of interarrival times the CDF tells us P(T &lt;= t) and the survival function tells us P(T &gt; t) 2) The Survival function at a value x is 1 - CDF(x) . waitTime = 6 lam = 6 totalTime = 60 print(&quot;P (wait &gt; {} mins) = &quot;.format(waitTime), math.exp(-(lam/totalTime)*waitTime)) #Now trying with scipy stats waitObj = stats.expon(scale = totalTime/lam) print(&quot;Scipy: P (wait &gt; {} mins) = &quot;.format(waitTime), waitObj.sf(waitTime)) . P (wait &gt; 6 mins) = 0.5488116360940264 Scipy: P (wait &gt; 6 mins) = 0.5488116360940265 . For a $ lambda$ of 6, we can expect to wait more than 6 minutes for the event 54.8% of the time . #Lets plot the exponential distribution to see what it looks like timeInt = 60 arr = np.arange(0,timeInt,1) fig, ax = plt.subplots(figsize=(18, 8)) lam = 6 waitObj = stats.expon(scale = totalTime/lam) sns.lineplot(x = arr,y = waitObj.sf(arr), label = &quot;SF: Interarrival for $ lambda$ = {}&quot;.format(lam)) plt.axvline(waitObj.mean(), color=&#39;red&#39;) plt.axvline(waitObj.median(), color=&#39;green&#39;) print(&quot;Mean =&quot;, waitObj.mean()) print(&quot;Median =&quot;, waitObj.median()) plt.text(waitObj.mean() - 1.2,0,&#39;Mean&#39;,rotation=90, fontsize=15) plt.text(waitObj.median() - 1.2,0,&#39;Median&#39;,rotation=90, fontsize=15) . Mean = 10.0 Median = 6.931471805599453 . Text(5.731471805599453, 0, &#39;Median&#39;) . So what is this telling us? The mean (red line) is greater than the median (green line). A $ lambda$ of 6/hr implies 1 event every 10 minutes which lines up with the mean. However as per this distribution, 50% of the time one would expect to wait 6.9 Minutes for the first event. . In the next step, I will create multiple large number of observations and plot their mean and median to evaluate the difference . fig, ax = plt.subplots(figsize=(18, 8)) sns.distplot([ waitObj.rvs(1000).mean() for x in range(1,1000)], kde=False, label=&quot;Mean&quot;) sns.distplot([ np.median(waitObj.rvs(1000)) for x in range(1,1000)], kde=False, label=&quot;Median&quot;) plt.legend() plt.show() . In a lot of tutorials that I have been reading on this, I have read about the memoryless property of the exponential distribution and the fact that on average you have to wait the same amount of time regardless of your arrival time. The alternate way for me to think about this is with the median. In 50% of the times, you will expect to wait less than the average waiting time or 5 out of 10 times the waiting time would be 7 minutes but the average is higher due to the &#39;long tail&#39; of this distribution. . Lets explore this a bit more . obs = randObj.exponential(scale = totalTime/lam, size = 10000) print(&quot;Observed Mean = &quot;, obs.mean()) print(&quot;Observed Median = &quot;, np.median(obs)) print(&quot;# of observations with wait time less than mean = {:.1%}&quot;.format(len(obs[obs &lt; obs.mean()])/len(obs))) print(&quot;Probability of wait time &lt;= mean waiting time = {:.1%}&quot;.format(waitObj.cdf(waitObj.mean()))) . Observed Mean = 9.972232742853603 Observed Median = 6.756204299960964 # of observations with wait time less than mean = 63.8% Probability of wait time &lt;= mean waiting time = 63.2% . On a sample of 100000, in 63% of the cases the observer has to wait less than the mean waiting time to observe the event as confirmed by the CDF as well. . Finally let&#39;s turn attention to simulating the arrival times of k events/time by using the Gamma distribution. . There are two ways I can think about this. 1) The exponential distribution (interarrival times) is memoryless, meaning that the time to next arrival is not dependent on the previous arrival. So for a $ lambda$ of 6/hour generating 6 rvs for an exponential rv and thaking their cumulative sum should give us an instance of arrivals for 1 hour 2) Using the gamma distribution to generate this value . Lets first plot the gamma distribution for a $ lambda$ of 6/hr and k from 1 to 6 . arr = np.linspace(0,60,num=100) lam = 6 fig, ax = plt.subplots(figsize=(18, 8)) for i in range (1,7): x = stats.gamma(a = i, loc = 0, scale = 60/lam) sns.lineplot(x = arr,y = x.pdf(arr), label = &quot;k = {}&quot;.format(i)) plt.axvline(x.mean(), color = &quot;blue&quot;, alpha = 0.5, lw = 1, linestyle = &#39;--&#39;) plt.text(x.mean() - 1.2,0.05,&#39;Mean of k = {}&#39;.format(i),rotation=90, fontsize=15) plt.show() . Next, lets see generate arrival times for arrivals 1 to 6 via numpy and evaluate the mean of these distributions . #Generating an array of 6x10,000 for poisson rvs obs = randObj.exponential(scale = totalTime/lam, size = (10000, lam)) #Calculating the cumulative sum of the arrrival times arrivals = obs.cumsum(axis = 1) for i in range (0,6): print(&quot;Mean arrival time for {}th arrival = {}&quot;.format(i+1, arrivals[:,i].mean())) . Mean arrival time for 1th arrival = 10.032815807109468 Mean arrival time for 2th arrival = 20.15253780295285 Mean arrival time for 3th arrival = 30.128105315395583 Mean arrival time for 4th arrival = 40.26826075653025 Mean arrival time for 5th arrival = 50.397751948215095 Mean arrival time for 6th arrival = 60.46905511047107 . As expected, the mean arrival time for a large number of rvs lines up with the expected value. To close this off, lets draw out some random observations of the arrival times and see what that looks like. . Although I am done with the first phase of poisson exploration, the following is just for my learning. I have a graph in my mind that I want to plot so might as well . #starting with a singular observation #Step 1: Draw out a random arrival time observation sampleDraw = 10 print(arrivals[randObj.randint(0,len(arrivals))]) xlabels = [&#39;1st&#39;, &#39;2nd&#39;,&#39;3rd&#39;,&#39;4th&#39;,&#39;5th&#39;,&#39;6th&#39;] ylabels = np.arange(1,sampleDraw+1,1) #Get 10 random entries from the arrivals array x = randObj.randint(0,len(arrivals), size = sampleDraw) #reshape to a single array of x sample = arrivals[x] fig, ax = plt.subplots(figsize=(10, 15)) for i in range (0,6): #y = np.ones(6)*(i+1) #print(y) #print(sample[i]) plt.scatter(x = sample[:,i], y = ylabels, marker=&#39;${}$&#39;.format(i+1),s=100) plt.axvline(60, color = &quot;blue&quot;, alpha = 0.5, lw = 1, linestyle = &#39;--&#39;) plt.ylim(0,sampleDraw+1) plt.yticks(ylabels) plt.ylabel(&quot;Iteration&quot;, rotation=90) plt.xlabel(&quot;Time (mins)&quot;) plt.show() . [ 5.60810941 5.63600358 21.53003487 27.60503203 28.73231595 31.52059577] . Running this graph a few times, yields a lot of intersting combonations with some in which the first occurance is actually after the 1st hour . This concludes my fist exploration of poisson. In the next notebook, I will aim to apply this to a more practical problem .",
            "url": "https://mindfire83.github.io/deepdive/jupyter/2021/01/21/Exploring-Poisson.html",
            "relUrl": "/jupyter/2021/01/21/Exploring-Poisson.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mindfire83.github.io/deepdive/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mindfire83.github.io/deepdive/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mindfire83.github.io/deepdive/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}